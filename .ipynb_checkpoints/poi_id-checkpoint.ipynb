{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminxiao/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named feature_format",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-43567505fee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../tools/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfeature_format\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeatureFormat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetFeatureSplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtester\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdump_classifier_and_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named feature_format"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "# features_list is a list of my selected features\n",
    "# all_features is a list for exploration\n",
    "#features_list = ['poi', 'bonus', 'expenses', 'bon_plus_expenses', 'bon_sal_ratio', \\\n",
    "#                'to_msg_ratio', 'from_msg_ratio']\n",
    "\n",
    "features_list = ['poi', 'bon_plus_expenses', 'exercised_stock_options', \n",
    "                'total_payments']\n",
    "all_features = ['poi', 'salary', 'bonus', 'long_term_incentive',\\\n",
    "                'deferred_income', 'expenses', 'total_payments', \\\n",
    "                'exercised_stock_options', 'restricted_stock', 'other', 'to_messages', \\\n",
    "                'email_address', 'from_poi_to_this_person', 'from_messages', \\\n",
    "                'from_this_person_to_poi', 'shared_receipt_with_poi', 'to_msg_ratio', \\\n",
    "                'from_msg_ratio', 'bon_plus_expenses', 'bon_sal_ratio'] \n",
    "\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "df = pd.DataFrame.from_records(list(data_dict.values()))\n",
    "employees = pd.Series(list(data_dict.keys()))\n",
    "\n",
    "# set the index of df to be the employees series:\n",
    "df.set_index(employees, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create another working dataframe to make new features \n",
    "df_new = df.apply(lambda x: pd.to_numeric(x, errors='coerce')).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The follow are my created features\n",
    "\n",
    "# from_msg_ratio is ratio messages received from poi to total messages received\n",
    "df_new['to_msg_ratio'] = df_new.from_this_person_to_poi.divide(df_new.to_messages, axis = 'index')\n",
    "\n",
    "# create to_msg_ratio by dividing from_this_person_to_poi from to_messages\n",
    "df_new['from_msg_ratio'] = df_new.from_poi_to_this_person.divide(df_new.from_messages, axis = 'index')\n",
    "\n",
    "# create a new feature by adding expenses and bonus together\n",
    "df_new['bon_plus_expenses'] = df_new['bonus'].add(df_new['expenses'], axis = 'index')\n",
    "# new feature of bonus to salary ratio\n",
    "\n",
    "df_new['bon_sal_ratio'] = df_new['bonus'].divide(df_new['salary'], axis = 'index')\n",
    "# new feature of bonus to expenses ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many missing values are in each column\n",
    "print df_new.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill NaN with 0 where operations created NaN in some rows\n",
    "df_new.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after you create features, the column names will be your new features\n",
    "# create a list of column names:\n",
    "new_features_list = df_new.columns.values\n",
    "print new_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2: Remove outliers\n",
    "# plot salary vs bonus as first step of outlier detection, visually\n",
    "%matplotlib inline\n",
    "x = df_new['salary']\n",
    "y = df_new['bonus']\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('Salary')\n",
    "plt.ylabel('Bonus')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Highest bonus value: \" + str(df_new['bonus'].max())\n",
    "print \"Highest salary value: \" + str(df_new['salary'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify the highest bonus and salary values to see what is going on \n",
    "df_new.sort_values(['bonus', 'salary'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removed row \"TOTAL\" because it's not a proper data point, as in it's not an employee\n",
    "df_new.drop(['TOTAL'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary from the dataframe\n",
    "df_dict = df_new.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many data points are left in my data\n",
    "print \"Number of data points: \" + str(len(my_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "# Created one function for exploration then another for use after feature selection\n",
    "exploration_data = featureFormat(my_dataset, all_features, sort_keys = True)\n",
    "labels_exploration, features_exploration = targetFeatureSplit(exploration_data)\n",
    "\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First one tried is RandomForestClassifier\n",
    "rfc_exploration = RandomForestClassifier()\n",
    "rfc_exploration = rfc_exploration.fit(features_exploration, labels_exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Also trying a decision tree classifier because tree classifiers make sense here\n",
    "dc_exploration = DecisionTreeClassifier()\n",
    "dc_exploration= dc_exploration.fit(features_exploration, labels_exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function appends the feature and according importance value from tree\n",
    "# classifier to a list to view more neatly\n",
    "rfc_impt = []\n",
    "dc_impt = []\n",
    "\n",
    "def input_impt(impt_list, features_list, impts):\n",
    "    for i in range(len(impts)):\n",
    "        impt_list.append( (features_list[i], impts[i]) )\n",
    "    \n",
    "    impt_list.sort(key = lambda tup: tup[1], reverse = True)\n",
    "    \n",
    "    return impt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call previous function to append and sort feature importances \n",
    "input_impt(rfc_impt, all_features[1:], rfc_exploration.feature_importances_)\n",
    "input_impt(dc_impt, all_features[1:], dc_exploration.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"RandomForestClassifier importances values: \"\n",
    "for item in rfc_impt:\n",
    "    print item[0] + \" : \" + str(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"DecisionTreeClassifier importances values: \"\n",
    "for item in dc_impt:\n",
    "    print item[0] + \" : \" + str(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign to new classifiers after choosing features\n",
    "\n",
    "rfc = rfc_exploration.fit(features, labels)\n",
    "dc = dc_exploration.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info:\n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# straified cv for parameters, 100 fold, and shuffled\n",
    "best_cv = StratifiedShuffleSplit(n_splits = 100, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random_state is to bring consistency to results\n",
    "# results to best_params_ were inconsistent before adding random_state\n",
    "# If you uncomment to run these lines of code, it may take a while.\n",
    "# Added start and end times to see how long this all takes because\n",
    "# this exhaustive method has been taking forever. \n",
    "# Both CV settings are set to optimize for f1 to get better precision and recall\n",
    "# It took me 94 minutes to run rfc and about 2 to run decisiontreeclassifier\n",
    "\n",
    "#start_gridcv_rfc = time.time()\n",
    "#rfc_param_grid = {'n_estimators': [1,2, 3, 10, 100], \n",
    "#                 'min_samples_split': [2, 3, 5],\n",
    "#                 'random_state': [42],\n",
    "#                 'max_features': [1, 2, 3],\n",
    "#                 'max_depth' : [2, 3, 5, 10, 50],\n",
    "#                 'min_samples_leaf': [1, 2, 3, 10]\n",
    "#                 }\n",
    "\n",
    "#grid_cv_rfc = GridSearchCV(estimator = rfc, param_grid = rfc_param_grid, cv = best_cv,\n",
    "#                          n_jobs = 5, scoring = 'f1')\n",
    "#grid_cv_rfc.fit(features, labels)\n",
    "#end_gridcv_rfc = time.time()\n",
    "#print \"Minutes elapsed: \" + str((float(end_gridcv_rfc - start_gridcv_rfc) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearchcv for decisiontreeclassifier\n",
    "# The list comprehension for max_features is just to make the feature selection\n",
    "# process easier on me.\n",
    "\n",
    "start_gridcv_dc = time.time()\n",
    "dc_param_grid = {'min_samples_split' : [2, 3, 4, 5, 10, 50],\n",
    "                 'max_features' : [x for x in range(1, len(features_list))],\n",
    "                 'min_samples_leaf': [1, 2, 3, 10, 20],\n",
    "                'random_state' : [42]\n",
    "                }\n",
    "grid_cv_dc = GridSearchCV(estimator = dc, param_grid = dc_param_grid, cv = best_cv,\n",
    "                         n_jobs = 5, scoring = 'f1')\n",
    "grid_cv_dc.fit(features, labels)\n",
    "end_gridcv_dc = time.time()\n",
    "print \"Minutes elapsed: \" + str((float(end_gridcv_dc - start_gridcv_dc) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print classification_report(labels_train, grid_cv_rfc.best_estimator_.predict(features_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print classification_report(labels_train, grid_cv_dc.best_estimator_.predict(features_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print classification_report(labels_test, grid_cv_rfc.best_estimator_.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print classification_report(labels_test, grid_cv_dc.best_estimator_.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grid_cv_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign clf to classifer chosen after testing with tester.py\n",
    "# Parameters are selected from GridSearchCV's best_params_ attributes\n",
    "\n",
    "#clf = RandomForestClassifier(min_samples_split = 2, n_estimators = 3,\n",
    "#                            random_state = 42, max_depth = 50, min_samples_leaf = 1,\n",
    "#                            max_features = 3)\n",
    "#clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_cv_dc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters are selected from GridSearchCV's best_params_ attributes\n",
    "# I ended up choosing DecisionTreeClassifier because it performed better with\n",
    "# precision and recall in tester.py\n",
    "clf = DecisionTreeClassifier(min_samples_split = 2, random_state = 42,\n",
    "                            max_features = 2, min_samples_leaf = 1)\n",
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_pred = clf.predict(features_test)\n",
    "f1_score(labels_test, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
