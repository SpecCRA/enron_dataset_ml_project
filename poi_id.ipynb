{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bean/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.cross_validation import train_test_split\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "# features_list is a list of my selected features\n",
    "# finance_features has finance features I'm interested in\n",
    "# email_features is a list of email features\n",
    "features_list = ['poi', 'bonus', 'expenses', 'bon_plus_expenses', 'bon_sal_ratio', \\\n",
    "                'to_msg_ratio', 'from_msg_ratio']\n",
    "finance_features = ['poi', 'salary', 'bonus', 'long_term_incentive', \\\n",
    "                 'deferred_income', 'expenses', 'total_payments', \\\n",
    "                 'exercised_stock_options', 'restricted_stock', 'other'] \n",
    "email_features = ['poi', 'to_messages', 'email_address', \n",
    "                 'from_poi_to_this_person', 'from_messages', \\\n",
    "                 'from_this_person_to_poi', 'shared_receipt_with_poi', \\\n",
    "                 'to_msg_ratio', 'from_msg_ratio']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "df = pd.DataFrame.from_records(list(data_dict.values()))\n",
    "employees = pd.Series(list(data_dict.keys()))\n",
    "\n",
    "# set the index of df to be the employees series:\n",
    "df.set_index(employees, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in poi: 128\n",
      "Number of missing values in bonus: 64\n",
      "Number of missing values in expenses: 51\n",
      "Created feature:  bon_plus_expenses\n",
      "Created feature:  bon_sal_ratio\n",
      "Created feature:  to_msg_ratio\n",
      "Created feature:  from_msg_ratio\n"
     ]
    }
   ],
   "source": [
    "# Find how many missing values are in my selected features\n",
    "for feature in features_list:\n",
    "    try:\n",
    "        print \"Number of missing values in \" + str(feature) + \": \" + str(df[feature].value_counts(dropna=False)[0])\n",
    "    except:\n",
    "        print \"Created feature: \", str(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create another working dataframe to make new features \n",
    "\n",
    "df_new = df.apply(lambda x: pd.to_numeric(x, errors='coerce')).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from_msg_ratio is ratio messages received from poi to total messages received\n",
    "df_new['to_msg_ratio'] = df_new.from_this_person_to_poi.divide(df_new.to_messages, axis = 'index')\n",
    "\n",
    "# create to_msg_ratio by dividing from_this_person_to_poi from to_messages\n",
    "df_new['from_msg_ratio'] = df_new.from_poi_to_this_person.divide(df_new.from_messages, axis = 'index')\n",
    "\n",
    "# create a new feature by adding expenses and bonus together\n",
    "df_new['bon_plus_expenses'] = df_new['bonus'].add(df_new['expenses'], axis = 'index')\n",
    "# new feature of bonus to salary ratio\n",
    "\n",
    "df_new['bon_sal_ratio'] = df_new['bonus'].divide(df_new['salary'], axis = 'index')\n",
    "# new feature of bonus to expenses ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill NaN with 0 where operations created NaN in some rows\n",
    "df_new.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary from the dataframe\n",
    "df_dict = df_new.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bonus', 'deferral_payments', 'deferred_income', 'director_fees',\n",
       "       'email_address', 'exercised_stock_options', 'expenses',\n",
       "       'from_messages', 'from_poi_to_this_person',\n",
       "       'from_this_person_to_poi', 'loan_advances', 'long_term_incentive',\n",
       "       'other', 'poi', 'restricted_stock', 'restricted_stock_deferred',\n",
       "       'salary', 'shared_receipt_with_poi', 'to_messages',\n",
       "       'total_payments', 'total_stock_value', 'to_msg_ratio',\n",
       "       'from_msg_ratio', 'bon_plus_expenses', 'bon_sal_ratio'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after you create features, the column names will be your new features\n",
    "# create a list of column names:\n",
    "new_features_list = df_new.columns.values\n",
    "new_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2: Remove outliers\n",
    "\n",
    "# From the mini project, we have to remove the one outlier called \"TOTAL\" as \n",
    "# a spreadsheet quirk\n",
    "\n",
    "df_new.drop(['TOTAL'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15654285  0.23536453  0.13178955  0.13761229  0.15467439  0.18401639]\n"
     ]
    }
   ],
   "source": [
    "# First one tried is RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 25, min_samples_split = 3, \n",
    "                            random_state = 2)\n",
    "rfc = rfc.fit(features, labels)\n",
    "\n",
    "print rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02553311  0.46042969  0.38345358  0.01896745  0.03282828  0.07878788]\n"
     ]
    }
   ],
   "source": [
    "# Also trying a decision tree classifier because tree classifiers make sense here\n",
    "\n",
    "dc = DecisionTreeClassifier()\n",
    "dc = dc.fit(features, labels)\n",
    "\n",
    "print dc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using selectkbest to do feature selection\n",
    "\n",
    "selection = SelectKBest(chi2, k = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info:\n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# straified cv for parameters, 100 fold and shuffled\n",
    "best_cv = StratifiedShuffleSplit(n_splits = 100, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bean/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/bean/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/bean/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/bean/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/bean/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# look up parameters to use for cross validation here \n",
    "# random_state is to bring consistency to results\n",
    "# results to best_params_ was variable before adding that parameter\n",
    "# If you uncomment to run these lines of code, it may take a while\n",
    "# Added start and end times to see how long this all takes because\n",
    "# this exhaustive method has been taking forever\n",
    "\n",
    "start_gridcv_rfc = time.time()\n",
    "rbc_param_grid = {'n_estimators': [1,2, 3, 10, 100], \n",
    "                 'min_samples_split': [2, 3, 5],\n",
    "                 'random_state': [2],\n",
    "                 'max_features': [1, 2, 3],\n",
    "                 'max_depth' : [2, 3, 5, 10, 50],\n",
    "                 'min_samples_leaf': [1, 2, 3, 10]\n",
    "                 }\n",
    "\n",
    "grid_cv_rfc = GridSearchCV(estimator = rfc, param_grid = rbc_param_grid, cv = best_cv,\n",
    "                          n_jobs = 5, scoring = 'f1')\n",
    "grid_cv_rfc.fit(features, labels)\n",
    "end_gridcv_rfc = time.time()\n",
    "print \"Minutes elapsed: \" + str((float(end_gridcv_rfc - start_gridcv_rfc) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearchcv for decisiontreeclassifier\n",
    "\n",
    "start_gridcv_dc = time.time()\n",
    "\n",
    "dc_param_grid = {'min_samples_split' : [2, 3, 4, 5, 10, 50],\n",
    "                 'max_features' : [1, 2, 3, 4, 'auto', 'sqrt', 'log2'],\n",
    "                 'min_samples_leaf': [1, 2, 3, 10, 20],\n",
    "                'random_state' : [2]\n",
    "                }\n",
    "grid_cv_dc = GridSearchCV(estimator = dc, param_grid = dc_param_grid, cv = best_cv,\n",
    "                         n_jobs = 5, scoring = 'f1')\n",
    "grid_cv_dc.fit(features, labels)\n",
    "\n",
    "end_gridcv_dc = time.time()\n",
    "\n",
    "print \"Minutes elapsed: \" + str((float(end_gridcv_dc - start_gridcv_dc) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans cross validation\n",
    "\n",
    "kf_rbc = KFold(n_splits = 10, shuffle=True, random_state = 2)\n",
    "kf_rbc.split(features, labels)\n",
    "print kf_rbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for train_indicies, test_indicies in kf_rbc:\n",
    "#    features_train = [data[ii] for ii in train_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print classification_report(labels_train, grid_cv_dc.best_estimator_.predict(features_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print classification_report(labels_test, grid_cv_dc.best_estimator_.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print classification_report(labels_train, grid_cv_rfc.best_estimator_.predict(features_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print classification_report(labels_test, grid_cv_rfc.best_estimator_.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_cv_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign clf to classifer chosen after testing\n",
    "\n",
    "clf = RandomForestClassifier(min_samples_split = 5, n_estimators = 3,\n",
    "                            random_state = 2, max_depth = 50, min_samples_leaf = 1,\n",
    "                            max_features = 1)\n",
    "\n",
    "clf = make_pipeline(selection, clf)\n",
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv_dc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(min_samples_split = 2, random_state = 2,\n",
    "                            max_features = 3, min_samples_leaf = 1)\n",
    "\n",
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
