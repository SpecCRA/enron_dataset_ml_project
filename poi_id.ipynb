{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bean/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.cross_validation import train_test_split\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "# features_list is a list of my selected features\n",
    "# all_features is a list for exploration\n",
    "features_list = ['poi', 'bonus', 'expenses', 'bon_plus_expenses', 'bon_sal_ratio', \\\n",
    "                'to_msg_ratio', 'from_msg_ratio']\n",
    "all_features = ['poi', 'salary', 'bonus', 'long_term_incentive', \\\n",
    "                'deferred_income', 'expenses', 'total_payments', \\\n",
    "                'exercised_stock_options', 'restricted_stock', 'other', 'to_messages', \\\n",
    "                'email_address', 'from_poi_to_this_person', 'from_messages', \\\n",
    "                'from_this_person_to_poi', 'shared_receipt_with_poi', 'to_msg_ratio', \\\n",
    "                'from_msg_ratio', 'bon_plus_expenses', 'bon_sal_ratio'] \n",
    "\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "df = pd.DataFrame.from_records(list(data_dict.values()))\n",
    "employees = pd.Series(list(data_dict.keys()))\n",
    "\n",
    "# set the index of df to be the employees series:\n",
    "df.set_index(employees, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in poi: 128\n",
      "Number of missing values in salary: 51\n",
      "Number of missing values in bonus: 64\n",
      "Number of missing values in long_term_incentive: 80\n",
      "Number of missing values in deferred_income: 97\n",
      "Number of missing values in expenses: 51\n",
      "Number of missing values in total_payments: 21\n",
      "Number of missing values in exercised_stock_options: 44\n",
      "Number of missing values in restricted_stock: 36\n",
      "Number of missing values in other: 53\n",
      "Number of missing values in to_messages: 60\n",
      "Number of missing values in email_address: 35\n",
      "Number of missing values in from_poi_to_this_person: 12\n",
      "Number of missing values in from_messages: 60\n",
      "Number of missing values in from_this_person_to_poi: 20\n",
      "Number of missing values in shared_receipt_with_poi: 60\n",
      "Created feature:  to_msg_ratio\n",
      "Created feature:  from_msg_ratio\n",
      "Created feature:  bon_plus_expenses\n",
      "Created feature:  bon_sal_ratio\n"
     ]
    }
   ],
   "source": [
    "# Find how many missing values are in all features\n",
    "for feature in all_features:\n",
    "    try:\n",
    "        print \"Number of missing values in \" + str(feature) + \": \" + str(df[feature].value_counts(dropna=False)[0])\n",
    "    except:\n",
    "        print \"Created feature: \", str(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create another working dataframe to make new features \n",
    "df_new = df.apply(lambda x: pd.to_numeric(x, errors='coerce')).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from_msg_ratio is ratio messages received from poi to total messages received\n",
    "df_new['to_msg_ratio'] = df_new.from_this_person_to_poi.divide(df_new.to_messages, axis = 'index')\n",
    "\n",
    "# create to_msg_ratio by dividing from_this_person_to_poi from to_messages\n",
    "df_new['from_msg_ratio'] = df_new.from_poi_to_this_person.divide(df_new.from_messages, axis = 'index')\n",
    "\n",
    "# create a new feature by adding expenses and bonus together\n",
    "df_new['bon_plus_expenses'] = df_new['bonus'].add(df_new['expenses'], axis = 'index')\n",
    "# new feature of bonus to salary ratio\n",
    "\n",
    "df_new['bon_sal_ratio'] = df_new['bonus'].divide(df_new['salary'], axis = 'index')\n",
    "# new feature of bonus to expenses ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill NaN with 0 where operations created NaN in some rows\n",
    "df_new.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bonus', 'deferral_payments', 'deferred_income', 'director_fees',\n",
       "       'email_address', 'exercised_stock_options', 'expenses',\n",
       "       'from_messages', 'from_poi_to_this_person',\n",
       "       'from_this_person_to_poi', 'loan_advances', 'long_term_incentive',\n",
       "       'other', 'poi', 'restricted_stock', 'restricted_stock_deferred',\n",
       "       'salary', 'shared_receipt_with_poi', 'to_messages',\n",
       "       'total_payments', 'total_stock_value', 'to_msg_ratio',\n",
       "       'from_msg_ratio', 'bon_plus_expenses', 'bon_sal_ratio'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after you create features, the column names will be your new features\n",
    "# create a list of column names:\n",
    "new_features_list = df_new.columns.values\n",
    "new_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 2: Remove outliers\n",
    "\n",
    "# From the mini project, we have to remove the one outlier called \"TOTAL\" as \n",
    "# a spreadsheet quirk\n",
    "df_new.drop(['TOTAL'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once outliers are removed, data values should be scaled\n",
    "# Email ratios definitely don't match bonus and expenses scales\n",
    "df_new_scaled = (df_new- df_new.min()) / (df_new.max() - df_new.min())\n",
    "# Some of these may have created NaNs in the dataset\n",
    "# Fill the NaN with 0 again\n",
    "df_new_scaled.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary from the dataframe\n",
    "df_dict = df_new_scaled.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "# Created one function for exploration then another for use after feature selection\n",
    "exploration_data = featureFormat(my_dataset, all_features, sort_keys = True)\n",
    "labels_exploration, features_exploration = targetFeatureSplit(exploration_data)\n",
    "\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First one tried is RandomForestClassifier\n",
    "rfc_exploration = RandomForestClassifier()\n",
    "rfc_exploration = rfc_exploration.fit(features_exploration, labels_exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Also trying a decision tree classifier because tree classifiers make sense here\n",
    "dc_exploration = DecisionTreeClassifier()\n",
    "dc_exploration= dc_exploration.fit(features_exploration, labels_exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function appends the feature and according importance value from tree\n",
    "# classifier to a list to view more neatly\n",
    "rfc_impt = []\n",
    "dc_impt = []\n",
    "\n",
    "def input_impt(impt_list, features_list, impts):\n",
    "    for i in range(len(impts)):\n",
    "        impt_list.append( (features_list[i], impts[i]) )\n",
    "    \n",
    "    impt_list.sort(key = lambda tup: tup[1], reverse = True)\n",
    "    \n",
    "    return impt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bon_plus_expenses', 0.29502004557122657),\n",
       " ('total_payments', 0.14544656276939744),\n",
       " ('restricted_stock', 0.12049154306372167),\n",
       " ('exercised_stock_options', 0.1178661654327829),\n",
       " ('other', 0.10873640794900637),\n",
       " ('from_poi_to_this_person', 0.084572761738116065),\n",
       " ('expenses', 0.062514833950841148),\n",
       " ('salary', 0.042286380869058032),\n",
       " ('to_msg_ratio', 0.023065298655849813),\n",
       " ('bonus', 0.0),\n",
       " ('long_term_incentive', 0.0),\n",
       " ('deferred_income', 0.0),\n",
       " ('to_messages', 0.0),\n",
       " ('email_address', 0.0),\n",
       " ('from_messages', 0.0),\n",
       " ('from_this_person_to_poi', 0.0),\n",
       " ('shared_receipt_with_poi', 0.0),\n",
       " ('from_msg_ratio', 0.0),\n",
       " ('bon_sal_ratio', 0.0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call previous function to append and sort feature importances \n",
    "input_impt(rfc_impt, all_features[1:], rfc_exploration.feature_importances_)\n",
    "input_impt(dc_impt, all_features[1:], dc_exploration.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier importances values: \n",
      "exercised_stock_options : 0.122471023375\n",
      "total_payments : 0.118718714094\n",
      "from_msg_ratio : 0.107951535971\n",
      "bon_plus_expenses : 0.095008081976\n",
      "long_term_incentive : 0.0591623036555\n",
      "expenses : 0.0574439350125\n",
      "salary : 0.0569913402422\n",
      "from_this_person_to_poi : 0.0518028470662\n",
      "restricted_stock : 0.0498157137159\n",
      "other : 0.0494126575681\n",
      "bonus : 0.048397993605\n",
      "shared_receipt_with_poi : 0.0390989668465\n",
      "bon_sal_ratio : 0.0350574095139\n",
      "from_poi_to_this_person : 0.0349101659826\n",
      "to_msg_ratio : 0.0313264178462\n",
      "deferred_income : 0.0257570805662\n",
      "from_messages : 0.0111766484915\n",
      "to_messages : 0.00549716447217\n",
      "email_address : 0.0\n"
     ]
    }
   ],
   "source": [
    "print \"RandomForestClassifier importances values: \"\n",
    "for item in rfc_impt:\n",
    "    print item[0] + \" : \" + str(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier importances values: \n",
      "bon_plus_expenses : 0.295020045571\n",
      "total_payments : 0.145446562769\n",
      "restricted_stock : 0.120491543064\n",
      "exercised_stock_options : 0.117866165433\n",
      "other : 0.108736407949\n",
      "from_poi_to_this_person : 0.0845727617381\n",
      "expenses : 0.0625148339508\n",
      "salary : 0.0422863808691\n",
      "to_msg_ratio : 0.0230652986558\n",
      "bonus : 0.0\n",
      "long_term_incentive : 0.0\n",
      "deferred_income : 0.0\n",
      "to_messages : 0.0\n",
      "email_address : 0.0\n",
      "from_messages : 0.0\n",
      "from_this_person_to_poi : 0.0\n",
      "shared_receipt_with_poi : 0.0\n",
      "from_msg_ratio : 0.0\n",
      "bon_sal_ratio : 0.0\n"
     ]
    }
   ],
   "source": [
    "print \"DecisionTreeClassifier importances values: \"\n",
    "for item in dc_impt:\n",
    "    print item[0] + \" : \" + str(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign to new classifiers after choosing features\n",
    "\n",
    "rfc = rfc_exploration.fit(features, labels)\n",
    "dc = dc_exploration.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info:\n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# straified cv for parameters, 100 fold, and shuffled\n",
    "best_cv = StratifiedShuffleSplit(n_splits = 100, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random_state is to bring consistency to results\n",
    "# results to best_params_ were inconsistent before adding random_state\n",
    "# If you uncomment to run these lines of code, it may take a while.\n",
    "# Added start and end times to see how long this all takes because\n",
    "# this exhaustive method has been taking forever. \n",
    "# Both CV settings are set to optimize for f1 to get better precision and recall\n",
    "# It took me 78 minutes to run rfc and about 3 to run decisiontreeclassifier\n",
    "\n",
    "#start_gridcv_rfc = time.time()\n",
    "#rfc_param_grid = {'n_estimators': [1,2, 3, 10, 100], \n",
    "#                 'min_samples_split': [2, 3, 5],\n",
    "#                 'random_state': [2],\n",
    "#                 'max_features': [1, 2, 3],\n",
    "#                 'max_depth' : [2, 3, 5, 10, 50],\n",
    "#                 'min_samples_leaf': [1, 2, 3, 10]\n",
    "#                 }\n",
    "\n",
    "#grid_cv_rfc = GridSearchCV(estimator = rfc, param_grid = rfc_param_grid, cv = best_cv,\n",
    "#                          n_jobs = 5, scoring = 'f1')\n",
    "#grid_cv_rfc.fit(features, labels)\n",
    "#end_gridcv_rfc = time.time()\n",
    "#print \"Minutes elapsed: \" + str((float(end_gridcv_rfc - start_gridcv_rfc) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bean/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/bean/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/bean/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/bean/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/bean/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minutes elapsed: 2.40754206578\n"
     ]
    }
   ],
   "source": [
    "# gridsearchcv for decisiontreeclassifier\n",
    "\n",
    "start_gridcv_dc = time.time()\n",
    "dc_param_grid = {'min_samples_split' : [2, 3, 4, 5, 10, 50],\n",
    "                 'max_features' : [1, 2, 3, 4, 'auto', 'sqrt', 'log2'],\n",
    "                 'min_samples_leaf': [1, 2, 3, 10, 20],\n",
    "                'random_state' : [2]\n",
    "                }\n",
    "grid_cv_dc = GridSearchCV(estimator = dc, param_grid = dc_param_grid, cv = best_cv,\n",
    "                         n_jobs = 5, scoring = 'f1')\n",
    "grid_cv_dc.fit(features, labels)\n",
    "end_gridcv_dc = time.time()\n",
    "print \"Minutes elapsed: \" + str((float(end_gridcv_dc - start_gridcv_dc) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bean/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/bean/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/bean/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minutes elapsed: 0.349845747153\n"
     ]
    }
   ],
   "source": [
    "# Try RandomSearchCV instead as an option for reviewer\n",
    "from scipy.stats import randint as sp_randint\n",
    "start_rcv_dc = time.time()\n",
    "param_dist = { 'min_samples_split': sp_randint(2,10),\n",
    "              'max_features' : sp_randint(1,5),\n",
    "              'min_samples_leaf': sp_randint(1,5),\n",
    "              'random_state': [2]\n",
    "    \n",
    "}\n",
    "rcv_dc = RandomizedSearchCV(estimator = dc, param_distributions = param_dist, \n",
    "                           cv = best_cv, scoring = 'f1', n_jobs = 3, n_iter = 20)\n",
    "rcv_dc.fit(features_test, labels_test)\n",
    "end_rcv_dc = time.time()\n",
    "print \"Minutes elapsed: \" + str((float(end_rcv_dc - start_rcv_dc) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print classification_report(labels_train, grid_cv_rfc.best_estimator_.predict(features_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00        70\n",
      "        1.0       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       1.00      1.00      1.00        81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(labels_train, grid_cv_dc.best_estimator_.predict(features_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print classification_report(labels_test, grid_cv_rfc.best_estimator_.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00        28\n",
      "        1.0       1.00      1.00      1.00         7\n",
      "\n",
      "avg / total       1.00      1.00      1.00        35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(labels_test, grid_cv_dc.best_estimator_.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grid_cv_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign clf to classifer chosen after testing with tester.py\n",
    "# Parameters are selected from GridSearchCV's best_params_ attributes\n",
    "\n",
    "#clf = RandomForestClassifier(min_samples_split = 5, n_estimators = 3,\n",
    "#                            random_state = 2, max_depth = 50, min_samples_leaf = 1,\n",
    "#                            max_features = 1)\n",
    "#clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 3,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'random_state': 2}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv_dc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 7,\n",
       " 'random_state': 2}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv_dc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=3, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=2, splitter='best')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters are selected from GridSearchCV's best_params_ attributes\n",
    "# I ended up choosing DecisionTreeClassifier because it performed better with\n",
    "# precision and recall in tester.py\n",
    "clf = DecisionTreeClassifier(min_samples_split = 2, random_state = 2,\n",
    "                            max_features = 3, min_samples_leaf = 1)\n",
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
