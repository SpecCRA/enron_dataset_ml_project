{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fnil\fcharset0 HelveticaNeue-Bold;\f1\fnil\fcharset0 HelveticaNeue;\f2\fnil\fcharset0 Monaco;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;\red133\green133\blue133;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c100000\c100000\c100000;\cssrgb\c59216\c59216\c59216;
}
\margl1440\margr1440
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\b\fs40 \cf2 \cb3 \expnd0\expndtw0\kerning0
\up0 \nosupersub \ulnone \outl0\strokewidth0 \strokec2 Machine Learning Free Response\
\pard\pardeftab720\partightenfactor0

\f1\b0\fs22 \cf2 \cb3 \strokec2 \
\pard\pardeftab720\partightenfactor0

\f0\b \cf2 \cb3 \strokec2 1. Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those? \
\
\pard\pardeftab720\partightenfactor0

\f1\b0 \cf2 \cb3 \strokec2 Enron, an American energy company, is famous for its bankruptcy and tremendous audit failure. In this project, we are given a dataset from Enron to detect fraud among certain people with financial and email data.\
\
We are using machine learning in order to classify all employees in the dataset to either be a person of interest or not. Machine learning is useful in doing this because of its many classifiers and methods, feature input, and speed. \
\
The only outlier I chose to remove was the entry \'93Total\'94 because it is a spreadsheet quirk that shouldn\'92t be used for training or testing data. I left everything else in the dataset because we are more than likely looking for outliers as persons of interest here. \
\pard\pardeftab720\partightenfactor0

\f0\b \cf2 \cb3 \strokec2 \
2. What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset \'97 explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you sed an automated feature selection like SelectKBest, please report the feature scores and reasons for your choice of parameter values. \
\
\pard\pardeftab720\partightenfactor0

\f1\b0 \cf2 \cb3 \strokec2 I ended up using bonus, expenses, bonus plus expenses, bonus to salary ratio, and email message ratios. The email message ratios are ones I created that used \'93from this person to person of interest\'94 and divided by the total \'93total to messages.\'94 The other is a ratio of \'93from person of interest to this person\'94 to \'93total from messages.\'94 \
\
\
Feature selection started by assessing how many missing values were in each category. Then I looked at RandomForestClassifier\'92s and DecisionTreeClassifier\'92s attribute, feature_importances_, compared the two, and went over the meanings of the finance features in the provided pdf file. \
\
Bonus plus expenses, bonus to salary ratio, and the two email ratio features were created by me. On their own, bonus and expenses were rated highly in feature importances. On its own, salary was not as significant in the classifiers. I decided to try different operations with bonuses and expenses to see if something would work better to predict a person of interest. The email ratios are taken directly from the feature selection lesson.\
\uc0\u8232 After removing outliers, features were scaled because email ratios are already between 0 and 1. Bonus, expenses, and other financial data could be in the millions. So my chosen features definitely should have been scaled properly.\
\
The following is print out of feature_importances_ : \
\
\pard\pardeftab720\sl320\partightenfactor0

\f0\b\fs20 \cf2 \cb3 \strokec2 RandomForestClassifier importances values:
\f1\b0 \cf2 \cb3 \strokec2  \
bon_plus_expenses : 0.156880870948\
shared_receipt_with_poi : 0.0938805010798\
exercised_stock_options : 0.0886586914315\
expenses : 0.0721349739674\
deferred_income : 0.0655302194077\
salary : 0.0632444836457\
other : 0.0628088716837\
bonus : 0.0620416757912\
to_msg_ratio : 0.0531902794314\
long_term_incentive : 0.0386696056055\
bon_sal_ratio : 0.0382817047049\
restricted_stock : 0.0381147583232\
from_messages : 0.0361053237201\
total_payments : 0.0346255019437\
from_msg_ratio : 0.0342954058409\
from_this_person_to_poi : 0.0240696856379\
to_messages : 0.0195355588321\
from_poi_to_this_person : 0.0179318880052\
email_address : 0.0\
\

\f0\b \cf2 \cb3 \strokec2 DecisionTreeClassifier importances values: \
\pard\pardeftab720\sl320\partightenfactor0

\f1\b0 \cf2 \cb3 \strokec2 bon_plus_expenses : 0.262259717535\
total_payments : 0.178206890805\
restricted_stock : 0.120491543064\
exercised_stock_options : 0.117866165433\
long_term_incentive : 0.108736407949\
from_poi_to_this_person : 0.0845727617381\
expenses : 0.0625148339508\
to_messages : 0.0422863808691\
to_msg_ratio : 0.0230652986558\
salary : 0.0\
bonus : 0.0\
deferred_income : 0.0\
other : 0.0\
email_address : 0.0\
from_messages : 0.0\
from_this_person_to_poi : 0.0\
shared_receipt_with_poi : 0.0\
from_msg_ratio : 0.0\
bon_sal_ratio : 0.0\
\pard\pardeftab720\sl320\partightenfactor0

\f2\fs24 \cf4 \cb3 \strokec4 \
\
\pard\pardeftab720\partightenfactor0

\f0\b\fs22 \cf2 \cb3 \strokec2 \
3. What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms? \
\
\pard\pardeftab720\partightenfactor0

\f1\b0 \cf2 \cb3 \strokec2 I ended up with DecisionTreeClassifier. It provided the best precision and recall scores together after tuning. It also worked much faster, so I had more opportunities to explore its parameters within the dataset. \
\
I tried RandomForestClassifier and AdaBoostClassifier with DecisionTreeClassifier as its base estimator. AdaBoost didn\'92t do anything special in my parameters besides take a lot longer. After tuning by GridSearchCV, RandomForestClassifier performed better in both accuracy (0.82) and precision (about 0.55!), but its recall was below 0.3. My DecisionTreeClassifier ended up with accuracy, precision, and recall scores of 0.78217, 0.34588, and 0.34450, respectively. \
\pard\pardeftab720\partightenfactor0

\f0\b \cf2 \cb3 \strokec2 \
4. What does it mean to tune the parameters of an algorithm, and what can happen if you don\'92t do this well? How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune \'97 if this is the case for the one you picked, identify and briefly explain how you would have done it for a model that was not your final choice or a different model that does utilize parameter tuning e.g. a decision tree classifier). \
\pard\pardeftab720\partightenfactor0

\f1\b0 \cf2 \cb3 \strokec2 \
Tuning the parameters of an algorithm is adjusting how the algorithm works on a more polished level. If you don\'92t tune your parameters, you can either get a classifier that is overfitted to a set of training data or one that is too general that it\'92s useless to set loose. \
\
For a tree classifier, I tuned its minimum samples split, minimum number of samples to be a leaf node, and its maximum number of features at each split. I added all these to a bunch of different options with GridSearchCV and picked the best parameters based on that. I continued to add parameters one by one and checking the evaluation scores until I was satisfied with my results. \
\pard\pardeftab720\partightenfactor0

\f0\b \cf2 \cb3 \strokec2 \
5. What is validation, and what\'92s a classic mistake you can make if you do it wrong? How did you validate your analysis? \
\
\pard\pardeftab720\partightenfactor0

\f1\b0 \cf2 \cb3 \strokec2 Validation is a way to test your trained model. It gives an estimate of its performance and checks for overfitting. If you don\'92t do validation properly, your classier may end up with a classifier that is perfectly fine for one set of training data but flops on another set. \
\
I chose to use GridSearchCV as my primary way of validating my classifier\'92s parameters. I have also included a little bit of RandomizedSearchCV which I should have used for my RandomForestClassifier. The RandomForestClassifier cross validation is going to take a long, long time to run. \
\pard\pardeftab720\partightenfactor0

\f0\b \cf2 \cb3 \strokec2 \
6. Give at least 2 evaluation metrics and your average performance for each of them. Explain an interpretation of your metrics that says something human-understandable about your algorithm\'92s performance. \
\
\pard\pardeftab720\partightenfactor0

\f1\b0 \cf2 \cb3 \strokec2 In this project, we used both precision and recall as evaluation metrics. \
\
Precision is the ratio of true positives to the sum of true positives and false positives. It is a measure of how well the classifier found positives in the dataset. The higher the score, the better the classifier performed. \
\
Recall is is the ratio of true positives to the sum of true positives and false negatives. It tells the ratio of how may correct results the classifier found in relation to how many correct results existed in the dataset. Ideally, the recall score should be 1 or close to 1. Recall is sometimes called sensitivity. }