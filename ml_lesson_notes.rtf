{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 Naive Bayes Notes:\

\b0 Prior probability - your one before testing\
Posterior probability - probability using test evidence and prior probability\
For classification: \
	- Good for text learning\
	- Labels based on differing probabilities\
	- Problem: ignores word order\
Strengths:\
	- Easy to implement\
	- Fast!\
Weaknesses:\
	- Doesn\'92t work for phrases\
\

\b SVM, Support Vector Machines:\

\b0 Inputs data and outputs a line that separates the data\
Works by MARGIN - distance between points\
Works to minimize margin\
Can be nonlinear! But prone to overfitting \
C Parameter: controls trade off between smooth decision boundary and classifying training points correctly\
Gamma parameter: defines how far the influence of a single training example reaches\
	- if high, points close to decision boundary hold most weight\
	- if low, points further away have higher weight and result creates a more squiggly boundary\
Strengths:\
	- works well in sets where there is clear separation\
Weaknesses:\
	- bad for large data sets\
\

\b Decision Trees:\

\b0 For multiple linear splits\
Entropy - how a decision tree decides where to split the data\
	- 1.0 means evenly split data in a section\
	- 0.0 means uniform data\
\

\b PCA - Principal Components Analysis\

\b0 Transforms input features as principal components\
Preprocessing step\
Max number of PCs = number of input features\
When to use PCA:\
	- latent features driving patterns in data ( size of a room in sq ft and number of rooms ) \
	- dimensionality reduction \
		- representing multiple dimensions on 2D\
		- reduces noise by tossing lower priority PCs\
		- make other algorithms work better with fewer inputs}