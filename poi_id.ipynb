{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import pandas\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "# features_list has finance features\n",
    "# email_features is a list of email features\n",
    "features_list = ['poi', 'salary', 'bonus', 'long_term_incentive', \\\n",
    "                 'deferred_income', 'expenses', 'total_payments', \\\n",
    "                 'exercised_stock_options', 'restricted_stock', 'other'] \n",
    "email_features = ['poi', 'to_messages', 'email_address', \n",
    "                 'from_poi_to_this_person', 'from_messages', \\\n",
    "                 'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
    "# You will need to use more features\n",
    "\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "df = pandas.DataFrame.from_records(list(data_dict.values()))\n",
    "employees = pandas.Series(list(data_dict.keys()))\n",
    "\n",
    "# set the index of df to be the employees series:\n",
    "df.set_index(employees, inplace=True)\n",
    "\n",
    "# You will have code here to add columns, i.e. new features,\n",
    "# to the df, or remove rows, i.e. employees, from the df\n",
    "df_finance = df[features_list]\n",
    "df_email = df[email_features]\n",
    "\n",
    "# after you create features, the column names will be your new features\n",
    "# create a list of column names:\n",
    "new_features_list = df.columns.values\n",
    "\n",
    "\n",
    "# create a dictionary from the dataframe\n",
    "df_dict = df.to_dict('index')\n",
    "\n",
    "# compare the original dictionary \n",
    "# with the dictionary reconstructed from the dataframe:  \n",
    "print df_dict == data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in poi: 128\n",
      "Number of missing values in to_messages: 60\n",
      "Number of missing values in email_address: 35\n",
      "Number of missing values in from_poi_to_this_person: 12\n",
      "Number of missing values in from_messages: 60\n",
      "Number of missing values in from_this_person_to_poi: 20\n",
      "Number of missing values in shared_receipt_with_poi: 60\n"
     ]
    }
   ],
   "source": [
    "# Find how many missing values are in each email feature\n",
    "for feature in email_features:\n",
    "    print \"Number of missing values in \" + feature + \": \" + \\\n",
    "    str(df_email[feature].value_counts(dropna=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poi                          bool\n",
      "to_messages                object\n",
      "email_address              object\n",
      "from_poi_to_this_person    object\n",
      "from_messages              object\n",
      "from_this_person_to_poi    object\n",
      "shared_receipt_with_poi    object\n",
      "dtype: object\n",
      "METTS MARK              False\n",
      "BAXTER JOHN C           False\n",
      "ELLIOTT STEVEN          False\n",
      "CORDES WILLIAM R        False\n",
      "HANNON KEVIN P           True\n",
      "MORDAUNT KRISTINA M     False\n",
      "MEYER ROCKFORD G        False\n",
      "MCMAHON JEFFREY         False\n",
      "HORTON STANLEY C        False\n",
      "PIPER GREGORY F         False\n",
      "HUMPHREY GENE E         False\n",
      "UMANOFF ADAM S          False\n",
      "BLACHMAN JEREMY M       False\n",
      "SUNDE MARTIN            False\n",
      "GIBBS DANA R            False\n",
      "LOWRY CHARLES P         False\n",
      "COLWELL WESLEY           True\n",
      "MULLER MARK S           False\n",
      "JACKSON CHARLENE R      False\n",
      "WESTFAHL RICHARD K      False\n",
      "WALTERS GARETH W        False\n",
      "WALLS JR ROBERT H       False\n",
      "KITCHEN LOUISE          False\n",
      "CHAN RONNIE             False\n",
      "BELFER ROBERT           False\n",
      "SHANKMAN JEFFREY A      False\n",
      "WODRASKA JOHN           False\n",
      "BERGSIEKER RICHARD P    False\n",
      "URQUHART JOHN A         False\n",
      "BIBI PHILIPPE A         False\n",
      "                        ...  \n",
      "REYNOLDS LAWRENCE       False\n",
      "DIMICHELE RICHARD G     False\n",
      "BHATNAGAR SANJAY        False\n",
      "CARTER REBECCA C        False\n",
      "BUCHANAN HAROLD G       False\n",
      "YEAP SOON               False\n",
      "MURRAY JULIA H          False\n",
      "GARLAND C KEVIN         False\n",
      "DODSON KEITH            False\n",
      "YEAGER F SCOTT           True\n",
      "HIRKO JOSEPH             True\n",
      "DIETRICH JANET R        False\n",
      "DERRICK JR. JAMES V     False\n",
      "FREVERT MARK A          False\n",
      "PAI LOU L               False\n",
      "BAY FRANKLIN R          False\n",
      "HAYSLETT RODERICK J     False\n",
      "FUGH JOHN L             False\n",
      "FALLON JAMES B          False\n",
      "KOENIG MARK E            True\n",
      "SAVAGE FRANK            False\n",
      "IZZO LAWRENCE L         False\n",
      "TILNEY ELIZABETH A      False\n",
      "MARTIN AMANDA K         False\n",
      "BUY RICHARD B           False\n",
      "GRAMM WENDY L           False\n",
      "CAUSEY RICHARD A         True\n",
      "TAYLOR MITCHELL S       False\n",
      "DONAHUE JR JEFFREY M    False\n",
      "GLISAN JR BEN F          True\n",
      "Name: poi, Length: 146, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print df_email.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 2: Remove outliers\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info:\n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
